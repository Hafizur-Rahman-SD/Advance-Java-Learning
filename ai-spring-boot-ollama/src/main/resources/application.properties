spring.application.name=ai-spring-boot-ollama


server.address=0.0.0.0
server.port=8090


# Ollama server (default)
spring.ai.ollama.base-url=http://localhost:11434

# Use llama3 model
spring.ai.ollama.chat.options.model=llama3


# increase timeouts (important for first response)
spring.ai.ollama.client.connect-timeout=30s
spring.ai.ollama.client.read-timeout=180s